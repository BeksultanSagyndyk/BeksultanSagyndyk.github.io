---
layout: post
title: "Paper Review 5: Mistral 7B"
subtitle: "Unleashing Mistral 7B: The Game-Changing 7-Billion-Parameter Beast?"
gh-repo: BeksultanSagyndyk/BeksultanSagyndyk.github.io
gh-badge: [star, follow]
tags: [LLM, long input transformers]
comments: true
author: Beksultan Sagyndyk
---
![Mistral](https://github.com/BeksultanSagyndyk/BeksultanSagyndyk.github.io/assets/46630209/24c6dbd0-69f2-4698-8f84-7ff91d3c5a32)

**Mistral 7B** outperforms the best open 13B model (Llama 2) across all evaluated benchmarks, and the best released 34B model (Llama 1) in reasoning, mathematics, and code generation.

- Code: [Mistral GitHub](https://github.com/mistralai/mistral-src)
- Webpage: [Mistral News](https://mistral.ai/news/announcing-mistral-7b/)

**Key moments:**

1. Grouped-query attention (GQA) for faster inference.
2. Sliding window attention (SWA) to effectively handle sequences of arbitrary length with reduced inference cost.

## Architecture
Mistral - Transformer.

![Mistral Architecture](https://github.com/BeksultanSagyndyk/BeksultanSagyndyk.github.io/assets/46630209/029c6e88-4374-4ef5-819e-9b67b8775b63)

### Sliding Window Attention

Number of operations in vanilla attention is quadratic in the sequence length.

In SWA: each token can attend to at most W tokens from the previous layer (here, W = 3). But tokens outside the sliding window still influence next word prediction.

![Sliding Window Attention](https://github.com/BeksultanSagyndyk/BeksultanSagyndyk.github.io/assets/46630209/9d329656-dd96-4321-9948-172ce66c984f)

At each attention layer, information can move forward by W tokens. Hence, after k attention layers, information can move forward by up to k Ã— W tokens (figure 1).

### Rolling Buffer Cache

A fixed attention span means we can limit our cache size.

1. The cache has a fixed size of W (= 3 in the picture example).
2. The keys and values for timestep i are stored in position i mod W of the cache.
3. When the position i > W, past values in the cache are overwritten, and the size of the cache stops increasing.

![Rolling Buffer Cache](https://github.com/BeksultanSagyndyk/BeksultanSagyndyk.github.io/assets/46630209/91bf15a4-e8d8-464f-8ed5-611c6111f38f)

## System Prompt to Enforce Guardrails

The system prompt below is designed to guide the model in generating answers within specified guardrails, similar to the work done with Llama 2.

**Guiding Principles:**

- Always assist with care, respect, and truth.
- Respond with utmost utility while ensuring security.
- Avoid harmful, unethical, prejudiced, or negative content.
- Ensure replies promote fairness and positivity.
