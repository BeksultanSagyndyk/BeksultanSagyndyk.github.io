---
layout: post
title: "Paper review: Mistral 7B"
subtitle: "Unleashing Mistral 7B: The Game-Changing 7-Billion-Parameter Beast?"
gh-repo: BeksultanSagyndyk/BeksultanSagyndyk.github.io
gh-badge: [star, follow]
tags: [LLM, long input transformers]
comments: true
author: Beksultan Sagyndyk
---
![image](https://github.com/BeksultanSagyndyk/BeksultanSagyndyk.github.io/assets/46630209/24c6dbd0-69f2-4698-8f84-7ff91d3c5a32)

Mistral 7B outperforms the best open 13B
model (Llama 2) across all evaluated benchmarks, and the best released 34B
model (Llama 1) in reasoning, mathematics, and code generation. 
Key moments:

1) grouped-query attention (GQA) for faster inference
2) sliding window attention (SWA) to effectively handle sequences of arbitrary length with a
reduced inference cost.
