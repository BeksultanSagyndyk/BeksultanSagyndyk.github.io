---
layout: post
title: "Paper review: Mixtral of Experts"
subtitle: "Unleashing Mistral 7B: The Game-Changing 7-Billion-Parameter Beast?"
gh-repo: BeksultanSagyndyk/BeksultanSagyndyk.github.io
gh-badge: [star, follow]
tags: [LLM, long input transformers]
comments: true
author: Beksultan Sagyndyk
---
![Mixtral](<img width="543" alt="Screenshot 2024-01-30 at 13 29 18" src="https://github.com/BeksultanSagyndyk/BeksultanSagyndyk.github.io/assets/46630209/ff2bb5ca-02d5-4995-a445-41e79f61354c">)

**Mixtral 8x7B** a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts).

- Code: [Mistral GitHub](https://github.com/mistralai/mistral-src)
- Webpage: [Mistral News](https://mistral.ai/news/mixtral-of-experts/)
